# -*- coding: utf-8 -*-
"""IR_Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PzTgBQh8RDTeOx2Qjio3xPNIWq7eKZ4y
"""

from google.colab import drive
drive.mount('/content/drive')

cd drive/MyDrive/IR_Project

ls

!nvidia-smi

!pip install -q transformers
import pandas as pd
from tqdm import tqdm
import re
import nltk
import matplotlib.pyplot as plt
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.tokenize.casual import EMOTICON_RE
from nltk.tokenize import RegexpTokenizer as reg
from nltk.corpus import stopwords
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer
from transformers.pipelines.pt_utils import KeyDataset
from scipy.special import softmax
import urllib.request
import numpy as np
import csv

import torch
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(device)

def find_retweeted(tweet):
    '''This function will extract the twitter handles of retweed people'''
    return re.findall('(?<=RT\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)
    
def find_mentioned(tweet):
    '''This function will extract the twitter handles of people mentioned in the tweet'''
    return re.findall('(?<!RT\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)  

def find_hashtags(tweet):
    '''This function will extract hashtags'''
    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet) 

def remove_links(tweet):
    '''Takes a string and removes web links from it'''
    tweet = re.sub(r'http\S+', '', tweet) # remove http links
    tweet = re.sub(r'bit.ly/\S+', '', tweet) # rempve bitly links
    tweet = tweet.strip('[link]') # remove [links]
    return tweet

def remove_users(tweet):
    '''Takes a string and removes retweet and @user information'''
    tweet = re.sub('(RT\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet
    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at
    return tweet

def preprocess(files, keyword_set = None):
    files = list(files)
    preprocessed = []
    
    for file in files:
        file = remove_links(file)
        file = remove_users(file)
        preproc = file.lower()        

        found = False
        if(keyword_set is not None):
          for check in keyword_set:
              if(keyword_set is None):
                break
              if(check in preproc):
                found = True
                break
        
        preproc = re.sub('[^ $a-zA-Z0-9]','',preproc)
        # preproc = word_tokenize(preproc)
        preproc = preproc.split()
        
        # preproc = [word for word in preproc if not word in cache]
        new_preproc = []
        for word in preproc:
          new_preproc.append(word)

        if(not found and keyword_set is not None):
          new_preproc = None
        else:
          new_preproc = " ".join(new_preproc)

        preprocessed.append(new_preproc)
    
    return preprocessed

def parse_sentiment_output(output):
  labels=[]
  task='sentiment'
  mapping_link = f"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt"
  with urllib.request.urlopen(mapping_link) as f:
      html = f.read().decode('utf-8').split("\n")
      csvreader = csv.reader(html, delimiter='\t')
  labels = [row[1] for row in csvreader if len(row) > 1]
  label = []
  score = []

  for i in output:
    if('1' in i['label']):
      label.append(labels[1])
    elif('2' in i['label']):
      label.append(labels[2])
    else:
      label.append(labels[0])

    score.append(i['score'])

  return label, score
  
def get_sentiment_score(tweet_column):
  task='sentiment'
  device = "cuda:0" if torch.cuda.is_available() else "cpu"
  
  MODEL = f"cardiffnlp/twitter-roberta-base-{task}"
  tweet_list = list(tweet_column)

  sentiment_pipeline = pipeline("sentiment-analysis", model = MODEL, tokenizer = MODEL, device = 0)
  
  output = sentiment_pipeline(tweet_list)

  return output

def get_sentiment_score_mini_batch(tweet_column):
  task='sentiment'
  device = "cuda:0" if torch.cuda.is_available() else "cpu"
  
  MODEL = f"cardiffnlp/twitter-roberta-base-{task}"
  tweet_list = list(tweet_column)
  sentiment_pipeline = pipeline("sentiment-analysis", model = MODEL, tokenizer = MODEL, device = 0)
  output = []
  for out in tqdm(sentiment_pipeline(tweet_list, batch_size=64), total=len(tweet_list)):
    output.append(out)

  return output

"""# Sentiment Analysis after finetuning"""

avax_path = '/content/drive/MyDrive/IR_Project/final_preprocessed_tweet/Avalanche_combined_preproc.csv'
avax_data = pd.read_csv(avax_path)

avax_data.head()

avax_data.columns

avax_data['text'] = avax_data['text'].apply(str)

avax_required_data = avax_data[['id','text', 'favourite_count', 'retweet_count', 'created_at', 'user_id_str', 'user_name', 'user_followers_count', 'user_verified', 'hashtags', 'mentioned']]
crypto_context = ['live', 'bitcoin', 'moon', 'crypto', 'avax', 'bitcoin', 'bit', 'ethereum', 'eth', 'liquidity', 'ecosystem', 'hodl', 'pump', 'whale', 'currency', '$', 'blockchain', 'block', 'chain', 'defi', 'nft', 'update', 'avalancheavax', 'token', 'project']
crypto_context = set(crypto_context)
avax_required_data['text'] = avax_required_data['text'].apply(str)
avax_preprocessed_text = preprocess(avax_required_data['text'], crypto_context)
avax_required_data['clean_text'] = avax_preprocessed_text
avax_required_data = avax_required_data.dropna()
avax_required_data.reset_index(inplace=True)
avax_required_data.to_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/avax_preproc.csv')

avax_data = pd.read_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/avax_preproc.csv')
avax_data['clean_text'] = avax_data['clean_text'].apply(str)
output = get_sentiment_score_mini_batch(avax_data['clean_text'])
avax_sentiment, avax_score = parse_sentiment_output(output)
avax_data['sentiment_label'] = avax_sentiment
avax_data['sentiment_score'] = avax_score
avax_data.to_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/avax_sentiment.csv')

doge_path = '/content/drive/MyDrive/IR_Project/final_preprocessed_tweet/DogeCoin_combined_preproc.csv'
doge_data = pd.read_csv(doge_path)
doge_required_data = doge_data[['id','text', 'favourite_count', 'retweet_count', 'created_at', 'user_id_str', 'user_name', 'user_followers_count', 'user_verified', 'hashtags', 'mentioned']]

doge_required_data['text'] = doge_required_data['text'].apply(str)
doge_preprocessed_text = preprocess(doge_required_data['text'])
doge_required_data['clean_text'] = doge_preprocessed_text
doge_required_data = doge_required_data.dropna()
doge_required_data.reset_index(inplace=True)
doge_required_data.to_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/doge_preproc.csv')

doge_data = pd.read_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/doge_preproc.csv')
doge_data['clean_text'] = doge_data['clean_text'].apply(str)
output = get_sentiment_score_mini_batch(doge_data['clean_text'])
doge_sentiment, doge_score = parse_sentiment_output(output)
doge_data['sentiment_label'] = doge_sentiment
doge_data['sentiment_score'] = doge_score
doge_data.to_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/doge_sentiment.csv')

solana_path = '/content/drive/MyDrive/IR_Project/final_preprocessed_tweet/Solana_combined_preproc.csv'
solana_data = pd.read_csv(solana_path)
solana_required_data = solana_data[['id','text', 'favourite_count', 'retweet_count', 'created_at', 'user_id_str', 'user_name', 'user_followers_count', 'user_verified', 'hashtags', 'mentioned']]

solana_required_data['text'] = solana_required_data['text'].apply(str)
solana_preprocessed_text = preprocess(solana_required_data['text'])
solana_required_data['clean_text'] = solana_preprocessed_text
solana_required_data = solana_required_data.dropna()
solana_required_data.reset_index(inplace=True)
solana_required_data.to_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/solana_preproc.csv')

solana_data = pd.read_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/solana_preproc.csv')
solana_data['clean_text'] = solana_data['clean_text'].apply(str)
output = get_sentiment_score_mini_batch(solana_data['clean_text'])
solana_sentiment, solana_score = parse_sentiment_output(output)
solana_data['sentiment_label'] = solana_sentiment
solana_data['sentiment_score'] = solana_score
solana_data.to_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/solana_sentiment.csv')

ripple_path = '/content/drive/MyDrive/IR_Project/final_preprocessed_tweet/Ripple_combined_preproc.csv'
ripple_data = pd.read_csv(ripple_path)
ripple_required_data = ripple_data[['id','text', 'favourite_count', 'retweet_count', 'created_at', 'user_id_str', 'user_name', 'user_followers_count', 'user_verified', 'hashtags', 'mentioned']]

ripple_required_data['text'] = ripple_required_data['text'].apply(str)
ripple_preprocessed_text = preprocess(ripple_required_data['text'])
ripple_required_data['clean_text'] = ripple_preprocessed_text
ripple_required_data = ripple_required_data.dropna()
ripple_required_data.reset_index(inplace=True)
ripple_required_data.to_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/ripple_preproc.csv')

ripple_data = pd.read_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/ripple_preproc.csv')
ripple_data['clean_text'] = ripple_data['clean_text'].apply(str)
output = get_sentiment_score_mini_batch(ripple_data['clean_text'])
ripple_sentiment, ripple_score = parse_sentiment_output(output)
ripple_data['sentiment_label'] = ripple_sentiment
ripple_data['sentiment_score'] = ripple_score
ripple_data.to_csv('/content/drive/MyDrive/IR_Project/preprocYashBhavya/ripple_sentiment.csv')

"""# Word Cloud"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS
crypto_context = ['shiba', 'solana', 'avax', 'matic','polygon' , 'ripple','live', 'bitcoin', 'moon', 'crypto', 'avax', 'bitcoin', 'bit', 'ethereum', 'eth', 'liquidity', 'ecosystem', 'hodl', 'pump', 'whale', 'currency', '$', 'blockchain', 'block', 'chain', 'defi', 'nft', 'update', 'avalancheavax', 'token', 'project']
crypto_context = set(crypto_context)

text = set()

for i in crypto_context:
  all = list(map(str,i.split(',')))
  for j in range(len(all)):
    all[j].strip()
  
  for j in all: text.add(j)

string = ''
for i in text:
  string += i + " "

wc = WordCloud(background_color = 'white', width = 1920, height = 1080)
wc.generate_from_text(string)

plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()

"""# EDA"""

import seaborn as sns

def value_vs_sentiment_graph(data):

  return 0

def retweets_vs_verified(data):
  return 0
solana = pd.read_csv('/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/solana_sentiment.csv')
solana.head(5)

solana.columns

crypto = ["solana","shiba","ripple","doge","matic","avax"]
for i in crypto:
  solana = pd.read_csv(f'/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/{i}_sentiment.csv',low_memory=False)
  solana = solana.sort_values(by="created_at")
  solana  = solana.iloc[1:-1,:]
  solana['polarity'] = np.zeros(len(solana))
  solana['req_price'] = (solana['high'] + solana['low'])/2

  solana['sent_copy'] = solana['sentiment_label']
  solana['polarity'][solana['sent_copy'] == 'neutral'] = 0.1
  solana['polarity'][solana['sent_copy'] == 'positive'] = 0.2
  solana['polarity'][solana['sent_copy'] == 'negative'] = 0
  solana['net_score'] = solana['polarity'] * solana['sentiment_score']
  solana['created_at'] = pd.to_datetime(solana['created_at'])

  req1= solana[['created_at', 'req_price']]
  req1 = req1.groupby(pd.Grouper(key='created_at', freq='10min')).mean()
  # req1['created_at'] = req1.index

  req = solana[['created_at', 'net_score']]
  req = req.groupby(pd.Grouper(key='created_at', freq='10min')).sum()
  # req['created_at'] = req.index

  req['req_price'] = req1['req_price']
  req['created_at'] = req.index
  req['net_score'] += 90

  req.plot(x='created_at', y=['req_price', 'net_score'])
  plt.legend(['Price of crypto', 'Weighted sentiment score'])
  plt.gcf().set_size_inches(20,10)
  plt.title(i+" Price vs Crypto Comparison")
  plt.show()

Dsolana = pd.read_csv(f'/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/avax_sentiment.csv')

# solana.dropna(axis=1)
solana = solana.sort_values(by="created_at")
solana
# for i in solana["created_at"]:
#   if(pd.to_datetime(i)):
#     print(i)
# print(solana['created_at'])
# solana['created_at'] = pd.to_datetime(solana['created_at'])

# solana/
# solana = solana.iloc[:-1,:]

"""# Retweet and favourite analysis"""

crypto = ["solana","shiba","ripple","doge","matic"]
for i in crypto:
  solana = pd.read_csv(f'/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/{i}_sentiment.csv',low_memory=False)
  print(solana.columns)
  retweet_verified = solana[['user_verified', 'retweet_count', 'favourite_count']]
  count = retweet_verified.user_verified.value_counts()
  verified_retweet = retweet_verified.groupby(pd.Grouper(key='user_verified')).sum()
  verified_retweet['user_verified_count'] = count
  verified_retweet['retweets_per_user'] = verified_retweet['retweet_count'] / verified_retweet['user_verified_count']
  verified_retweet['favourite_per_user'] = verified_retweet['favourite_count'] / verified_retweet['user_verified_count']
  verified_retweet['user_verified'] = verified_retweet.index
  verified_retweet = verified_retweet.drop(['retweet_count', 'favourite_count', 'user_verified_count'], axis = 1)

  X = ['retweets_per_user', 'favourite_per_user']
  Z = ['False', 'True']
  X_axis = np.arange(len(X))
  plt.xticks(X_axis, Z)
  plt.bar(X_axis - 0.2, verified_retweet['retweets_per_user'], 0.4, label = 'retweets per user')
  plt.bar(X_axis + 0.2, verified_retweet['favourite_per_user'], 0.4, label = 'favourite per user')
  plt.xlabel("User Verified")
  plt.title(i)
  plt.legend()
  plt.show()

