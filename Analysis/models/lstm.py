# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZcFERpg1ExR0dq9FrSSTX1U54fOkn9fn
"""

from google.colab import drive
drive.mount('/content/drive')

import sklearn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, SGDRegressor
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
import pickle
from tqdm import tqdm
import math
# from tsmoothie.smoother import KalmanSmoother
# sklearn.linear_model.SGDRegressor

crypto = "matic"
without_meta = 1
save = False

if(without_meta == 1):
  # results_path = f"/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/LSTM_Results/{crypto}/without_meta/"
  results_path = f"/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/final_LSTM_results/{crypto}/without_meta/"
else:
  # results_path = f"/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/LSTM_Results/{crypto}/with_meta/"
  results_path = f"/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/final_LSTM_results/{crypto}/with_meta/"

sentiment_df = pd.read_csv(f"/content/drive/MyDrive/IR_Project/final_sentiment_tweet_crypto_data/{crypto}_sentiment.csv")

sentiment_df.shape

sentiment_df.loc[sentiment_df["user_verified"] == 'True', "user_verified"] = 1
sentiment_df.loc[sentiment_df["user_verified"] == 'False', "user_verified"] = -1

sentiment_df["positive_score"] = 0
sentiment_df["negative_score"] = 0
sentiment_df["neutral_score"] = 0
sentiment_df["scaled_score"] = 0

import random

def scale_sentiment_score(label, score):
  if(label == 'positive'):
    # print('pos')
    scaled_score = score
  elif(label == 'negative'):
    scaled_score = -1 * score
  else:
    scaled_score = random.choice([-1, 1])*(0.5 - (0.5 * score))
  return scaled_score

sentiment_df['scaled_score'] = sentiment_df.apply(lambda x: scale_sentiment_score(x.sentiment_label, x.sentiment_score), axis=1)

# for i in tqdm(range(sentiment_df.shape[0])):
#   scaled_score = 0
#   label = sentiment_df.iloc[i]['sentiment_label']
#   score = sentiment_df.iloc[i]['sentiment_score']
#   if(label == 'positive'):
#     scaled_score = score
#   elif(label == 'negative'):
#     scaled_score = -1 * score
#   else:
#     scaled_score = random.choice([-1, 1])*(0.5 - (0.5 * score))
#   sentiment_df.loc[i, 'scaled_score'] = scaled_score

sentiment_df['positive_score'] = np.where(sentiment_df.sentiment_label == 'positive', sentiment_df.sentiment_score, 0)
sentiment_df['negative_score'] = np.where(sentiment_df.sentiment_label == 'negative', sentiment_df.sentiment_score, 0)
sentiment_df['neutral_score'] = np.where(sentiment_df.sentiment_label == 'neutral', sentiment_df.sentiment_score, 0)

sentiment_df['scaled_score']

sentiment_df['avg_price'] = (sentiment_df.high + sentiment_df.low)/2

sentiment_df_copy = sentiment_df.copy(deep=True)

enc = OneHotEncoder(sparse=False)
color_onehot = enc.fit_transform(sentiment_df_copy[['sentiment_label']])

sentiment_labels = pd.DataFrame(color_onehot, columns=list(enc.categories_[0]))
# verif_labels = pd.DataFrame(verif_onehot, columns=['not_verified', 'verified'])
# print(verif_labels)
sentiment_df_copy = pd.concat([sentiment_df_copy, sentiment_labels], axis=1)

sentiment_df_copy.columns

# sentiment_df_copy.drop(columns=['sentiment_score'])
sentiment_df_copy = sentiment_df_copy.reset_index()
sentiment_df_copy.dropna(inplace=True)

sentiment_df_copy['created_at'] = pd.to_datetime(sentiment_df_copy['created_at'])

sentiment_df_copy['year'] = sentiment_df_copy['created_at'].dt.year
sentiment_df_copy['month'] = sentiment_df_copy['created_at'].dt.month
sentiment_df_copy['day'] = sentiment_df_copy['created_at'].dt.day
sentiment_df_copy['hour'] = sentiment_df_copy['created_at'].dt.hour
sentiment_df_copy['minute'] = sentiment_df_copy['created_at'].dt.minute
sentiment_df_copy['second'] = sentiment_df_copy['created_at'].dt.second

# sentiment_df_copy['']

sentiment_df_copy['sentiment_val'] = 0

def apply_mapping(x):
  if(x == 'positive'):
    return 1
  elif(x == 'negative'):
    return -1
  elif(x == 'neutral'):
    return 0

# def apply_verif_
  
sentiment_df_copy['sentiment_mapping'] = sentiment_df_copy['sentiment_label'].apply(apply_mapping)

# import math
# def apply_log(x):
#   # print(x)
#   if(float(x) == 0):
#     return 0
#   return math.log10(float(x))
# # print(apply_log(100))

# sentiment_df_copy['log_rt'] = sentiment_df_copy['retweet_count'].apply(apply_log)
# sentiment_df_copy['log_fav'] = sentiment_df_copy['favourite_count'].apply(apply_log)
# # sentiment_df_copy['log_']

mm = MinMaxScaler()
sentiment_df_copy[['popularity_scale', 'favourite_scale', 'retweet_scale']] = mm.fit_transform(sentiment_df_copy[['user_followers_count', 'favourite_count', 'retweet_count']])

# list_of_features = ['favourite_count', 'retweet_count', 'user_followers_count', 'user_verified', 'negative', 'neutral', 'positive', 'positive_score', 'negative_score', 'neutral_score', 'high', 'low', 'open', 'volume_to', 'volume_from', 'close']
# list_of_features = ['log_rt', 'log_fav', 'sentiment_mapping', 'scaled_score', 'high', 'low', 'open', 'volume_to', 'volume_from', 'close']
if(without_meta == 1):
  list_of_features = ['scaled_score', 'sentiment_mapping', 'high', 'low', 'open', 'volume_to', 'volume_from', 'close']
else:
  list_of_features = ['popularity_scale', 'favourite_scale', 'retweet_scale', 'user_verified', 'scaled_score', 'sentiment_mapping', 'high', 'low', 'open', 'volume_to', 'volume_from', 'close']
# list_of_features = ['favourite_count', 'retweet_count', 'user_followers_count', 'user_verified', 'negative', 'neutral', 'positive', 'positive_score', 'negative_score', 'neutral_score', 'high', 'low', 'open', 'volume_to', 'volume_from', 'close', 'avg_price']

model_data = np.array(sentiment_df_copy[list_of_features])
model_data.shape

sent = sentiment_df_copy[['sentiment_score', 'sentiment_mapping', 'scaled_score','close','open', 'high', 'low']]
sent.corr(method ='pearson')

if(crypto == 'avax'):
    model_data = np.delete(model_data, 20642, 0)

model_data = model_data.astype(float)

model_data.shape

num_feats = model_data.shape[1] - 1

feature_data = model_data[:, :num_feats]
print(feature_data.shape)
y_price = model_data[:, num_feats:]
print(y_price.shape)

ss = StandardScaler()
# smoother = KalmanSmoother(component='level_longseason', 
#                           component_noise={'level':0.1, 'longseason':0.1}, 
#                           n_longseasons=365)
# X_ss = ss.fit_transform(feature_data)
# y_mm = mm.fit_transform(y_price) 

X_ss = feature_data
y_mm = y_price

test_size = 0.20
total_samples = X_ss.shape[0]
test_samples = int(test_size * total_samples)
train_samples = total_samples - test_samples

X_train = X_ss[:train_samples, :]
X_test = X_ss[train_samples:,:]
y_train = y_mm[:train_samples]
y_test = y_mm[train_samples:]

X_train = ss.fit_transform(X_train)
X_test = ss.transform(X_test)

# smoother.smooth(X_train)
# smoother.smooth(X_test)

y_train = mm.fit_transform(y_train)
y_test = mm.transform(y_test)
# X_train = normalize(X_train, norm='l2')
# X_test = normalize(X_test, norm='l2')

print("Training Shape", X_train.shape, y_train.shape)
print("Testing Shape", X_test.shape, y_test.shape)

import torch #pytorch
import torch.nn as nn
from torch.autograd import Variable

X_train_tensors = Variable(torch.Tensor(X_train))
X_test_tensors = Variable(torch.Tensor(X_test))

y_train_tensors = Variable(torch.Tensor(y_train))
y_test_tensors = Variable(torch.Tensor(y_test))

X_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))
X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))

print("Training Shape", X_train_tensors_final.shape, y_train_tensors.shape)
print("Testing Shape", X_test_tensors_final.shape, y_test_tensors.shape)

class LSTM1(nn.Module):
    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):
        super(LSTM1, self).__init__()
        self.num_classes = num_classes #number of classes
        self.num_layers = num_layers #number of layers
        self.input_size = input_size #input size
        self.hidden_size = hidden_size #hidden state
        self.seq_length = seq_length #sequence length

        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                          num_layers=num_layers, batch_first=True) #lstm
        self.fc_1 =  nn.Linear(num_layers *hidden_size, 128) #fully connected 1
        self.fc = nn.Linear(128, num_classes) #fully connected last layer

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(0.05)
    def forward(self,x):
        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state
        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state
        # Propagate input through LSTM
        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state
        hn = hn.view(-1, self.hidden_size * self.num_layers) #reshaping the data for Dense layer next
        # out = self.relu(hn)
        out = self.dropout(hn)
        out = self.fc_1(hn) #first Dense
        out = self.relu(out) #relu
        out = self.dropout(out)
        out = self.fc(out) #Final Output
        out = self.sigmoid(out)
        return out

num_epochs = 200 #1000 epochs
learning_rate = 8e-4 #0.001 lr

input_size = num_feats #number of features
hidden_size = 16 #number of features in hidden state
num_layers = 1 #number of stacked lstm layers

num_classes = 1 #number of output classes

lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]) #our lstm class

criterion = torch.nn.MSELoss()    # mean-squared error for regression
optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)
# optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=0.01)
# optimizer = torch.optim.SGD(lstm1.parameters(), lr=learning_rate, momentum=0.05, weight_decay=0.01)

train_loss = []
val_loss = []

for epoch in range(num_epochs):

  optimizer.zero_grad() #caluclate the gradient, manually setting to 0
  outputs = lstm1(X_train_tensors_final) #forward pass
  # optimizer.zero_grad() #caluclate the gradient, manually setting to 0
 
  # obtain the loss function
  loss = criterion(outputs, y_train_tensors)
  train_loss.append(loss.item()/len(y_train_tensors))
  loss.backward() #calculates the loss of the loss function
 
  optimizer.step() #improve from loss, i.e backprop
  print("Epoch: %d, loss: %1.5f" % (epoch, loss.item())) 
  output2 = lstm1(X_test_tensors_final) #forward pass
  #   # optimizer.zero_grad() #caluclate the gradient, manually setting to 0
  #   # obtain the loss function
  loss = criterion(output2, y_test_tensors)
  print("Validation Loss: ", loss.item()) 
  val_loss.append(loss.item()/len(y_test_tensors))

plt.plot(train_loss, label='train_loss')
plt.plot(val_loss, label='val_loss')
# plt.yticks(np.arange(0, 0.001, 1e-6))
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')
if(save == True):
  plt.savefig(results_path + 'loss_curve.jpg', bbox_inches='tight', facecolor='w')
plt.show()

X_test1 = Variable(torch.Tensor(X_test)) #converting to Tensors
y_test1 = Variable(torch.Tensor(y_test))
#reshaping the dataset
X_test1 = torch.reshape(X_test1, (X_test1.shape[0], 1, X_test1.shape[1]))

test_predict = lstm1(X_test1)#forward pass
data_predict = test_predict.data.numpy()#numpy conversion
dataY_plot = y_test1.data.numpy()

data_predict = mm.inverse_transform(data_predict) #reverse transformation
dataY_plot = mm.inverse_transform(dataY_plot)

# data_predict = [x - (0.02e-5) for x in data_predict]
plt.figure(figsize=(20,6)) #plotting
plt.axvline(x=200, c='r', linestyle='--') #size of the training set
# ax.set_ylim([ymin, ymax])

# plt.xlim([1, ])
# plt.ylim([1.2, 1.8])
# plt.yticks(np.arange(1.2, 1.8, 0.2))
# plt.xticks(np.arange(-3, 3, 0.25))
plt.plot(data_predict, label='Predicted Data [Test]') #predicted plot
plt.plot(dataY_plot, label='Actual Data [Test]') #actual plot
plt.title('Time-Series Prediction')
# plt.ylim((2.2e-5, 2.8e-5))
plt.legend()
plt.xlabel("Series")
plt.ylabel("Price")
if(save == True):
  plt.savefig(results_path + 'price_preds_test.jpg', bbox_inches='tight', facecolor='w')
plt.show()

# print(len(d))

#Full plot
data_train = mm.inverse_transform(y_train)
x = [i for i in range(len(data_train))]
last_x = x[len(x) - 1]
x2 = [i for i in range(last_x + 1, last_x + 1 + len(data_predict))]

# x.extend(x2)
plt.figure(figsize=(20,6))
plt.plot(x, data_train, color='blue', label='Actual Price [Train]')
plt.plot(x2, data_predict, color='red', label='Predicted Price')
plt.plot(x2, dataY_plot, color='yellow', label='Actual Price [Test]')
plt.legend()
plt.ylabel('Price')
plt.xlabel('Datapoint index')
if(save == True):
  plt.savefig(results_path + 'price_curve_whole.jpg', bbox_inches='tight', facecolor='w')
plt.show()

mses = []
maes = []
# rmses = 
for i in range(len(data_predict)):
  mse = (data_predict[i] - dataY_plot[i])**2
  mae = abs(data_predict[i] - dataY_plot[i])
  # print(f"pred = {data_predict[i]} | true = {dataY_plot[i]} | MSE = {mse}")
  mses.append(mse)
  maes.append(mae)
plt.plot([x for x in range(len(mses))], mses)
plt.show()

test_rmse = math.sqrt(sum(mses) / len(mses))
test_mae = (sum(maes) / len(maes))[0]
print(f"Test RMSE = {test_rmse}")
print(f"Test MAE = {test_mae}")

perc_errors = []
mean = 0
for i in range(len(data_predict)):
  mae_perc = ((abs(data_predict[i] - dataY_plot[i])) / dataY_plot[i] ) * 100
  perc_errors.append(mae_perc)
  mean += mae_perc

perc_errors = np.array(perc_errors)
mean_perc_error = np.mean(perc_errors)
max_perc_error = np.amax(perc_errors)
print(f"Mean error = {mean_perc_error}")
print(f"max : {max_perc_error}")
# print(perc_errors.shape)

print('R2 Score : ', metrics.r2_score(data_predict, dataY_plot))

if(save == True):
  stats_f = open(results_path + 'stats.txt', 'w')
  lines = [f'Stats for Crypto {crypto}\n',
          f'Model params : \n Hidden Layer size = {hidden_size}\n Learning_Rate = {learning_rate}\n Num epochs = {num_epochs}\n',
          f'Results : \n Test MAE = {test_mae}\n Test RMSE = {test_rmse} \n Mean percentage error = {mean_perc_error} \n Max perc error = {max_perc_error}'
          ]
  stats_f.writelines(lines)
  stats_f.close()